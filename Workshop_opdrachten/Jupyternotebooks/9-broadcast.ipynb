{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effiency\n",
    "\n",
    "In the map/reduce framework, a `map` is a very cheap operation, because it is executed in parallel on local data. A `reduce` is a very expensive operation, because all data has to be shuffle sorted and re-partitioned over reducers. For efficient processing, a few rules of thumb are: filter early, and avoid shuffle sort (reduce transformations like join, groupByKey).\n",
    "\n",
    "One way to avoid expensive shuffle sorts, is to replace `join` operations with `broadcast joins`. For large datasets, **the speedup can easily exceed 100x-1000x**, so the correct use of broadcast joins is vital for efficient processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast ##\n",
    "\n",
    "Broadcasting is the sending a set of data to each worker. When one table is small, it is much more efficient to join tables using a broadcast than to use a join operator. The first example shows the mechanism of broadcasting by turning one table into a Python dictionary, and then mapping the other table with a function that uses that dictionary. The additional cost is that the dictionary has to be send (*broadcasted*) to all workers, the gain is that the map can operate locally on the data and no reduce is required. Processing will be more efficient when the gain of broadcasting is greater than the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "productdata = \"\"\"\n",
    "Apple\tFruit\n",
    "Pear\tFruit\n",
    "Pizza\tFastFood\n",
    "Fries\tFastFood\n",
    "\"\"\"\n",
    "\n",
    "salesdata = \"\"\"\n",
    "Apple\t2000\n",
    "Apple\t1000\n",
    "Pizza\t3000\n",
    "Fries\t5000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = sc.parallelize(productdata.strip().split('\\n'))\n",
    "products = products.map(lambda x: tuple(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = sc.parallelize(salesdata.strip().split('\\n'))\n",
    "sales = sales.map(lambda x: tuple(x.split()))\n",
    "sales = sales.map(lambda x: (x[0], int(x[1])))\n",
    "sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Filtering ##\n",
    "\n",
    "First, we will use a *broadcast* as a lookup, much like an SQL subselect. In this case we want the sales records for all 'Fruit'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fruit = set( products.filter(lambda x: x[1] == 'Fruit').\\\n",
    "             map(lambda x: x[0]).collect() )\n",
    "fruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We queried for all products in the category 'Fruit' and obtained a set of keys that meet the requirements. We can then use the set of keys `fruit` to filter the `sales` RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales.filter(lambda x: x[0] in fruit).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Join ##\n",
    "\n",
    "Similarly, we can broadcast a dictionary to allow to append columns based on a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fruit = products.collectAsMap()\n",
    "fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales.map(lambda x: (x[0], x[1], fruit[x[0]])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Spark broadcast ##\n",
    "\n",
    "We can improve the broadcast by using Spark's broadcast function. The 'subtle' difference is that the data is send once to every node, instead of along with every task. Suppose we have a cluster of 100 nodes each with 8 cores (800 cores in total), and we partitioned the data in 2000 parts. Using Spark's broadcast will send the variable 100 times, while not using broadcast will send the data 2000 times.\n",
    "\n",
    "To use Spark broadcast, we need a dictionary (confusingly, also referred to as a map) which we can collect with a single statement using `collectAsMap` (alternatively, you can use `dict(foo.collect())`). The broadcast results in a `broadcast variable` that you can use in a function to do the lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fruit_bc = sc.broadcast(products.collectAsMap())\n",
    "sales.map(lambda x: (x[0], x[1], fruit_bc.value[x[0]])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
